{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqi1I9FVbcN9LHzEW/F2i5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spindouken/atlas-machine_learning/blob/main/reinforcement_learning%20/temporal_difference/Temporal_Difference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Temporal Difference"
      ],
      "metadata": {
        "id": "WGxbZ-U69RZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Monte Carlo\n",
        "<br>\n",
        "Write the function def monte_carlo(env, V, policy, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99):<br> that performs the Monte Carlo algorithm:<br>\n",
        "\n",
        "env is the openAI environment instance<br>\n",
        "V is a numpy.ndarray of shape (s,) containing the value estimate<br>\n",
        "policy is a function that takes in a state and returns the next action to take<br>\n",
        "episodes is the total number of episodes to train over<br>\n",
        "max_steps is the maximum number of steps per episode<br>\n",
        "alpha is the learning rate<br>\n",
        "Returns: V, the updated value estimate<br>\n",
        "```\n",
        "$ ./0-main.py\n",
        "[[ 0.81    0.9     0.4783  0.4305  0.3874  0.4305  0.6561  0.9   ]\n",
        " [ 0.9     0.729   0.5905  0.4783  0.5905  0.2824  0.2824  0.3874]\n",
        " [ 1.      0.5314  0.729  -1.      1.      0.3874  0.2824  0.4305]\n",
        " [ 1.      0.5905  0.81    0.9     1.     -1.      0.3874  0.6561]\n",
        " [ 1.      0.6561  0.81   -1.      1.      1.      0.729   0.5314]\n",
        " [ 1.     -1.     -1.      1.      1.      1.     -1.      0.9   ]\n",
        " [ 1.     -1.      1.      1.     -1.      1.     -1.      1.    ]\n",
        " [ 1.      1.      1.     -1.      1.      1.      1.      1.    ]]\n",
        "$\n",
        "```\n",
        "Repo:\n",
        "\n",
        "GitHub repository: atlas-machine_learning<br>\n",
        "Directory: reinforcement_learning/temporal_difference<br>\n",
        "File: 0-monte_carlo.py<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "18O8opeM8hgD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "0-monte_carlo.py"
      ],
      "metadata": {
        "id": "dnxujD2C8wuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def generate_episode(env, policy, max_steps):\n",
        "    \"\"\"\n",
        "    generate an episode by following the given policy\n",
        "\n",
        "    params:\n",
        "    env: the OpenAI Gym environment instance\n",
        "    policy: a function that takes in a state and returns the next action\n",
        "    max_steps: the maximum number of steps in the episode\n",
        "\n",
        "    returns a list of tuples, each containing: (state, reward)\n",
        "    \"\"\"\n",
        "    episode_data = []\n",
        "    state = env.reset()  # start a new episode and get the initial state\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        action = policy(state)  # determine the action based on the current state and policy\n",
        "        next_state, reward, terminate, _ = env.step(action)  # apply the action to the environment to get the next state and reward\n",
        "        episode_data.append((state, reward))  # store the state and reward in the episode data\n",
        "\n",
        "        if terminate or step > max_steps:\n",
        "            break  # end the episode if the environment says it's done\n",
        "\n",
        "        state = next_state  # move to the next state\n",
        "\n",
        "    return episode_data\n",
        "\n",
        "def monte_carlo(env, V, policy, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99):\n",
        "    \"\"\"\n",
        "    performs the Monte Carlo algorithm\n",
        "\n",
        "    params:\n",
        "    env: the OpenAI Gym environment instance\n",
        "    V: numpy array of shape (s,) containing the value estimate\n",
        "    policy: a function that takes in a state and returns the next action\n",
        "    episodes: total number of episodes to train over\n",
        "    max_steps: maximum number of steps per episode\n",
        "    alpha: the learning rate\n",
        "    gamma: the discount rate\n",
        "\n",
        "    returns V, the updated value estimate\n",
        "    \"\"\"\n",
        "    for episode in range(episodes):\n",
        "        episode_data = generate_episode(env, policy, max_steps)  # generate an episode data using the policy\n",
        "        episode_data = np.array(episode_data, dtype=int)  # convert episode data to numpy array for efficient processing\n",
        "\n",
        "        G = 0\n",
        "        # calculate return for each state in the episode, starting from the end\n",
        "        for s, r in episode_data[::-1]:\n",
        "            G = (gamma * G) + r  # update return with discount factor\n",
        "            # first visit monte carlo (update value only for first visit of the state in the episode)\n",
        "            if s not in episode_data[:episode, 0]:\n",
        "                V[s] += alpha * (G - V[s])  # update value estimate\n",
        "\n",
        "    return V.round(2)  # return the updated value estimates, rounded for readability\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import gym\n",
        "    import numpy as np\n",
        "    np.random.seed(0)\n",
        "\n",
        "    env = gym.make('FrozenLake8x8-v1')\n",
        "    LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
        "\n",
        "    def policy(s):\n",
        "        p = np.random.uniform()\n",
        "        if p > 0.5:\n",
        "            if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "                return RIGHT\n",
        "            elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "                return DOWN\n",
        "            elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
        "                return UP\n",
        "            else:\n",
        "                return LEFT\n",
        "        else:\n",
        "            if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "                return DOWN\n",
        "            elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "                return RIGHT\n",
        "            elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
        "                return LEFT\n",
        "            else:\n",
        "                return UP\n",
        "\n",
        "\n",
        "    V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64')\n",
        "    np.set_printoptions(precision=2)\n",
        "    env.seed(0)\n",
        "    print(monte_carlo(env, V, policy).reshape((8, 8)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tPvvHVVPz_H",
        "outputId": "79c78193-e7b8-432b-b8a5-a876ef6c753b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.9   0.73  0.66  0.73  0.9   0.9   0.59  0.53]\n",
            " [ 0.59  0.66  0.73  0.81  0.66  0.39  0.48  0.39]\n",
            " [ 0.66  0.25  0.35 -1.    1.    0.48  0.43  0.43]\n",
            " [ 0.9   0.43  0.28  0.59  0.9  -1.    0.48  0.48]\n",
            " [ 1.    0.73  0.59 -1.    1.    1.    0.73  0.73]\n",
            " [ 1.   -1.   -1.    1.    1.    1.   -1.    0.9 ]\n",
            " [ 1.   -1.    1.    1.   -1.    1.   -1.    1.  ]\n",
            " [ 1.    1.    1.   -1.    1.    1.    1.    1.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. TD(λ)\n",
        "<br>\n",
        "Write the function def td_lambtha(env, V, policy, lambtha, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99): that performs the TD(λ) algorithm:\n",
        "\n",
        "env is the openAI environment instance<br>\n",
        "V is a numpy.ndarray of shape (s,) containing the value estimate<br>\n",
        "policy is a function that takes in a state and returns the next action to take<br>\n",
        "lambtha is the eligibility trace factor<br>\n",
        "episodes is the total number of episodes to train over<br>\n",
        "max_steps is the maximum number of steps per episode<br>\n",
        "alpha is the learning rate<br>\n",
        "gamma is the discount rate<br>\n",
        "Returns: V, the updated value estimate<br>\n",
        "\n",
        "$ ./1-main.py\n",
        "<br>\n",
        "```\n",
        "[[ 0.5314  0.5905  0.3138  0.3138  0.6561  0.9     0.81    0.9   ]\n",
        " [ 0.5314  0.5905  0.4783  0.6561  0.5905  0.6561  0.6561  0.5314]\n",
        " [ 0.6561  0.729   0.5905 -1.      0.9     0.9     0.5905  0.3874]\n",
        " [ 0.729   0.81    0.81    0.9     1.     -1.      0.5314  0.4305]\n",
        " [ 0.5905  0.6561  0.81   -1.      1.      1.      0.729   0.4783]\n",
        " [ 0.9    -1.     -1.      1.      1.      1.     -1.      0.81  ]\n",
        " [ 1.     -1.      1.      1.     -1.      1.     -1.      1.    ]\n",
        " [ 0.9     0.81    1.     -1.      1.      1.      1.      1.    ]]\n",
        " ```"
      ],
      "metadata": {
        "id": "MV8Dfi6y87CU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def td_lambtha(env, V, policy, lambtha, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99):\n",
        "    \"\"\"\n",
        "    env is the openAI environment instance\n",
        "\n",
        "    params:\n",
        "    V: numpy.ndarray of shape (s,) containing the value estimate\n",
        "    polic: function that takes in a state and returns the next action to take\n",
        "    lambtha: eligibility trace factor\n",
        "    episodes: total number of episodes to train over\n",
        "    max_steps: maximum number of steps per episode\n",
        "    alpha: learning rate\n",
        "    gamma: discount rate\n",
        "\n",
        "    Returns: V, the updated value estimate\n",
        "    \"\"\"\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()  # reset environment to start a new episode\n",
        "        eligibilityTraces = np.zeros_like(V)\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            action = policy(state)  # determine action based on policy\n",
        "            next_state, reward, done, info = env.step(action)  # take action\n",
        "\n",
        "            # calculate TD Error\n",
        "            TD_error = reward + gamma * V[next_state] - V[state]\n",
        "            # print(f\"Episode {episode+1}, Step {step+1}, State: {state}, Action: {action}, Reward: {reward}, New State: {next_state}, TD Error: {TD_error}\")\n",
        "\n",
        "            # upd elegibility trace\n",
        "            eligibilityTraces[state] += 1\n",
        "\n",
        "            # update value estimate\n",
        "            V += alpha * TD_error * eligibilityTraces\n",
        "            # print(f\"Updated Value Function: {V}\")\n",
        "\n",
        "            # eligibility trace decay\n",
        "            eligibilityTraces *= gamma * lambtha\n",
        "            # print(f\"Updated Eligibility Trace: {eligibilityTraces}\")\n",
        "\n",
        "            if done:\n",
        "                # print(f\"Episode {episode+1} finished after {step+1} steps.\")\n",
        "                break\n",
        "            state = next_state\n",
        "\n",
        "    return V\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import gym\n",
        "    import numpy as np\n",
        "\n",
        "    np.random.seed(0)\n",
        "\n",
        "    env = gym.make('FrozenLake8x8-v1')\n",
        "    LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
        "\n",
        "    def policy(s):\n",
        "        p = np.random.uniform()\n",
        "        if p > 0.5:\n",
        "            if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "                return RIGHT\n",
        "            elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "                return DOWN\n",
        "            elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
        "                return UP\n",
        "            else:\n",
        "                return LEFT\n",
        "        else:\n",
        "            if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "                return DOWN\n",
        "            elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "                return RIGHT\n",
        "            elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
        "                return LEFT\n",
        "            else:\n",
        "                return UP\n",
        "\n",
        "    V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64')\n",
        "    np.set_printoptions(precision=4)\n",
        "    print(td_lambtha(env, V, policy, 0.9).reshape((8, 8)))"
      ],
      "metadata": {
        "id": "PA45o8MQnf4J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c71a58a3-14ad-435b-e979-f3a1b3ce36db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.861  -0.8645 -0.8442 -0.8304 -0.8075 -0.6999 -0.6783 -0.6727]\n",
            " [-0.8876 -0.8846 -0.8883 -0.887  -0.8548 -0.7688 -0.6772 -0.5907]\n",
            " [-0.9118 -0.9322 -0.9507 -1.     -0.9299 -0.8631 -0.7866 -0.6296]\n",
            " [-0.9261 -0.9348 -0.9436 -0.9843 -0.978  -1.     -0.811  -0.8148]\n",
            " [-0.9547 -0.9688 -0.9571 -1.     -0.9501 -0.8894 -0.8801 -0.8171]\n",
            " [-0.9704 -1.     -1.      0.3269 -0.9353 -0.8666 -1.     -0.3871]\n",
            " [-0.9711 -1.     -0.5221  0.0472 -1.     -0.7219 -1.      0.8954]\n",
            " [-0.9355 -0.8799 -0.8341 -1.      1.     -0.2502  0.801   1.    ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. SARSA(λ)\n",
        "<br>\n",
        "Write the function def sarsa_lambtha(env, Q, lambtha, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99, epsilon=1, min_epsilon=0.1, epsilon_decay=0.05): that performs SARSA(λ):\n",
        "\n",
        "env is the openAI environment instance<br>\n",
        "Q is a numpy.ndarray of shape (s,a) containing the Q table<br>\n",
        "lambtha is the eligibility trace factor<br>\n",
        "episodes is the total number of episodes to train over<br>\n",
        "max_steps is the maximum number of steps per episode<br>\n",
        "alpha is the learning rate<br>\n",
        "gamma is the discount rate<br>\n",
        "epsilon is the initial threshold for epsilon greedy<br>\n",
        "min_epsilon is the minimum value that epsilon should decay to<br>\n",
        "epsilon_decay is the decay rate for updating epsilon between episodes<br>\n",
        "Returns: Q, the updated Q table<br>\n",
        "```\n",
        "$ ./2-main.py\n",
        "[[0.5452 0.5363 0.6315 0.5329]\n",
        " [0.5591 0.6166 0.5316 0.5425]\n",
        " [0.5336 0.602  0.529  0.5463]\n",
        " [0.5475 0.5974 0.5362 0.5436]\n",
        " [0.5531 0.5693 0.6117 0.568 ]\n",
        " [0.6147 0.6011 0.6511 0.5966]\n",
        " [0.6472 0.6183 0.599  0.6176]\n",
        " [0.6334 0.6267 0.6519 0.634 ]\n",
        " [0.5571 0.5233 0.646  0.5867]\n",
        " [0.6456 0.5602 0.545  0.5321]\n",
        " [0.6303 0.53   0.5055 0.5394]\n",
        " [0.4495 0.4853 0.4384 0.5781]\n",
        " [0.5291 0.5351 0.5489 0.5821]\n",
        " [0.6182 0.6166 0.6186 0.6047]\n",
        " [0.6266 0.5832 0.6497 0.5645]\n",
        " [0.5369 0.3657 0.7081 0.4936]\n",
        " [0.5924 0.7393 0.5806 0.5818]\n",
        " [0.5621 0.7052 0.5681 0.5429]\n",
        " [0.6894 0.509  0.4663 0.5361]\n",
        " [0.2828 0.1202 0.2961 0.1187]\n",
        " [0.4457 0.4633 0.411  0.5208]\n",
        " [0.5899 0.6983 0.7595 0.5963]\n",
        " [0.7263 0.699  0.6698 0.6954]\n",
        " [0.6126 0.7508 0.4898 0.4768]\n",
        " [0.6615 0.5872 0.7568 0.5987]\n",
        " [0.5805 0.5433 0.5839 0.7284]\n",
        " [0.6236 0.6239 0.7243 0.5689]\n",
        " [0.6498 0.7383 0.6077 0.5422]\n",
        " [0.6334 0.6377 0.7003 0.6311]\n",
        " [0.8811 0.5813 0.8817 0.6925]\n",
        " [0.7557 0.7478 0.7796 0.7706]\n",
        " [0.6687 0.8253 0.65   0.5062]\n",
        " [0.6277 0.7568 0.6078 0.6561]\n",
        " [0.6366 0.6973 0.6338 0.7487]\n",
        " [0.7121 0.7965 0.7082 0.7455]\n",
        " [0.8965 0.3676 0.4359 0.8919]\n",
        " [0.7498 0.8535 0.3625 0.7401]\n",
        " [0.7681 0.7448 0.2974 0.837 ]\n",
        " [0.4996 0.6835 0.4382 0.8703]\n",
        " [0.8936 0.7053 0.4904 0.3181]\n",
        " [0.6677 0.7224 0.8078 0.6766]\n",
        " [0.9755 0.8558 0.0117 0.36  ]\n",
        " [0.73   0.1716 0.521  0.0543]\n",
        " [0.2466 0.0813 0.8518 0.2852]\n",
        " [0.3454 0.8602 0.7229 0.1075]\n",
        " [0.2801 0.7741 0.6684 0.288 ]\n",
        " [0.9342 0.614  0.5356 0.5899]\n",
        " [1.0137 0.391  0.4284 0.2431]\n",
        " [0.382  0.4696 0.4571 0.599 ]\n",
        " [0.2274 0.2544 0.058  0.4344]\n",
        " [0.3118 0.6755 0.4197 0.1796]\n",
        " [0.0247 0.0672 0.778  0.4537]\n",
        " [0.5366 0.8967 0.9903 0.2169]\n",
        " [0.6914 0.3132 0.0996 0.7817]\n",
        " [0.32   0.3835 0.5883 0.831 ]\n",
        " [0.629  1.3232 0.2735 0.8131]\n",
        " [0.2803 0.5022 0.5382 0.2851]\n",
        " [0.6295 0.6324 0.2997 0.2133]\n",
        " [0.5699 0.0643 0.2075 0.4247]\n",
        " [0.3742 0.4636 0.2776 0.5868]\n",
        " [0.8639 0.1175 0.5174 0.1321]\n",
        " [0.7169 0.3961 0.5654 0.1833]\n",
        " [0.1448 0.4881 0.3556 0.9404]\n",
        " [0.7653 0.7487 0.9037 0.0834]]\n",
        " ```"
      ],
      "metadata": {
        "id": "s-rWjGah8-wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_action(state, Q, epsilon):\n",
        "    \"\"\"\n",
        "    select an action based on the epsilon-greedy strategy\n",
        "\n",
        "    params:\n",
        "    state: The current state of the environment.\n",
        "    Q (numpy.ndarray): Q-table, containing the Q-values for each state-action pair\n",
        "    epsilon: probability of choosing a random action (exploration)\n",
        "\n",
        "    returns the selected action\n",
        "    \"\"\"\n",
        "    # if a random number is less than epsilon, pick a random action for exploration\n",
        "    #   otherwise, pick the best action from Q-table for exploitation\n",
        "    if np.random.uniform() < epsilon:\n",
        "        return np.random.randint(0, Q.shape[1])\n",
        "    else:\n",
        "        return np.argmax(Q[state, :])\n",
        "\n",
        "def update_epsilon(episode, min_epsilon, epsilon_decay):\n",
        "    \"\"\"\n",
        "    update epsilon value for the epsilon-greedy strategy\n",
        "\n",
        "    params:\n",
        "    episode: The current episode number.\n",
        "    min_epsilon: minimum value to which epsilon can decay\n",
        "    epsilon_decay: decay rate for epsilon\n",
        "\n",
        "    returns te updated epsilon value\n",
        "    \"\"\"\n",
        "    # update the value of epsilon after each episode\n",
        "    #   this gradually reduces epsilon, shifting from exploration to exploitation\n",
        "    # uses exponential decay for a smooth transition\n",
        "    return min_epsilon + (1 - min_epsilon) * np.exp(-1 * epsilon_decay * episode)\n",
        "\n",
        "def calculate_TD_error(reward, gamma, Q, state, action, next_state, next_action):\n",
        "    \"\"\"\n",
        "    calculate TD error (temporal difference)\n",
        "\n",
        "    params:\n",
        "    reward: reward received after taking the action\n",
        "    gamma: discount factor\n",
        "    Q (numpy.ndarray): Q-table\n",
        "    state: current state\n",
        "    action: action taken in the current state\n",
        "    next_state: next state after taking the action\n",
        "    next_action: The action taken in the next state\n",
        "\n",
        "    returns the calculated TD error\n",
        "    \"\"\"\n",
        "    # TD error is the difference between the estimated value and the observed value\n",
        "    #   it's a measure of how good the Q-value prediction was\n",
        "    return reward + gamma * Q[next_state, next_action] - Q[state, action]\n",
        "\n",
        "def sarsa_lambda(env, Q, lambda_value, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99, epsilon=1, min_epsilon=0.1, epsilon_decay=0.05):\n",
        "    \"\"\"\n",
        "    performs SARSA(λ)\n",
        "\n",
        "    env is the openAI environment instance\n",
        "    Q is a numpy.ndarray of shape (s,a) containing the Q table\n",
        "    lambtha is the eligibility trace factor\n",
        "    episodes is the total number of episodes to train over\n",
        "    max_steps is the maximum number of steps per episode\n",
        "    alpha is the learning rate\n",
        "    gamma is the discount rate\n",
        "    epsilon is the initial threshold for epsilon greedy\n",
        "    min_epsilon is the minimum value that epsilon should decay to\n",
        "    epsilon_decay is the decay rate for updating epsilon between episodes\n",
        "\n",
        "    Returns: Q, the updated Q table\n",
        "    \"\"\"\n",
        "    # eligibility traces are used to track the 'eligibility' of each state-action pair for learning\n",
        "    eligibilityTraces = np.zeros_like(Q)\n",
        "\n",
        "    # iterate over each episode\n",
        "    #   (an episode is a sequence of states, actions, and rewards, which ends with a terminal state)\n",
        "    for episode in range(episodes):\n",
        "        # reset environment for this episode\n",
        "        state = env.reset()\n",
        "\n",
        "        # choose the first action\n",
        "        action = select_action(state, Q, epsilon)\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            # take the action and observe the outcome\n",
        "            next_state, reward, terminate, _ = env.step(action)\n",
        "\n",
        "            # select next action using the epsilon-greedy policy, based on the new state\n",
        "            next_action = select_action(next_state, Q, epsilon)\n",
        "\n",
        "            # calculate the TD error and update the Q-table and eligibility traces\n",
        "            #   (represents the difference between the estimated value (Q-value)\n",
        "            #     and the observed value (reward + value of next state))\n",
        "            delta = calculate_TD_error(reward, gamma, Q, state, action, next_state, next_action)\n",
        "\n",
        "            # increase the eligibility trace for the current state-action pair\n",
        "            #  marks the state-action pair as eligible for learning\n",
        "            eligibilityTraces[state, action] += 1\n",
        "\n",
        "            # update Q-table using the TD error, learning rate (alpha), and eligibility traces\n",
        "            #  (adjusts the Q-value of the state-action pair, moving it closer to the expected long-term return)\n",
        "            Q += alpha * delta * eligibilityTraces\n",
        "\n",
        "            # decay the eligibility traces for all state-action pairs\n",
        "            #   (reduces the eligibility of state-action pairs over time, with a rate controlled by lambda_value and gamma)\n",
        "            #   (ensures that only recent state-action pairs have a significant effect on learning)\n",
        "            eligibilityTraces *= lambda_value * gamma\n",
        "\n",
        "            # update the current state and action to the next state and action,\n",
        "            #  moving the agent through the environment\n",
        "            state, action = next_state, next_action\n",
        "\n",
        "            # if the agent reached a terminal state exit loop for current episode\n",
        "            if terminate:\n",
        "                break\n",
        "\n",
        "        # update the exploration rate (epsilon) using the decay function\n",
        "        epsilon = update_epsilon(episode, min_epsilon, epsilon_decay)\n",
        "\n",
        "    # this Q-table represents the learned values for each state-action pair\n",
        "    return Q\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import gym\n",
        "    import numpy as np\n",
        "\n",
        "    np.random.seed(0)\n",
        "    env = gym.make('FrozenLake8x8-v1')\n",
        "    Q = np.random.uniform(size=(64, 4))\n",
        "    np.set_printoptions(precision=4)\n",
        "    print(sarsa_lambtha(env, Q, 0.9))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-tCnE4OIEUA",
        "outputId": "4ed13cad-06d1-4175-fc74-e9fba39d5252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.577  0.5661 0.7281 0.6128]\n",
            " [0.6914 0.5475 0.5336 0.6137]\n",
            " [0.6633 0.584  0.5316 0.5574]\n",
            " [0.5126 0.578  0.528  0.6215]\n",
            " [0.5814 0.6064 0.6452 0.6049]\n",
            " [0.6298 0.6188 0.7422 0.6403]\n",
            " [0.6588 0.6868 0.583  0.6272]\n",
            " [0.6272 0.7334 0.5934 0.6229]\n",
            " [0.7643 0.5776 0.5977 0.5683]\n",
            " [0.5646 0.6838 0.5756 0.5467]\n",
            " [0.5535 0.6873 0.5522 0.5964]\n",
            " [0.4974 0.578  0.4594 0.665 ]\n",
            " [0.5942 0.6644 0.6176 0.6292]\n",
            " [0.7087 0.6393 0.676  0.62  ]\n",
            " [0.6742 0.6645 0.7151 0.5975]\n",
            " [0.5651 0.4632 0.7753 0.5404]\n",
            " [0.5894 0.5998 0.7609 0.5806]\n",
            " [0.7465 0.6096 0.5601 0.5595]\n",
            " [0.7187 0.5315 0.4977 0.5087]\n",
            " [0.2828 0.1202 0.2961 0.1187]\n",
            " [0.4475 0.6715 0.463  0.4508]\n",
            " [0.7117 0.7488 0.8323 0.659 ]\n",
            " [0.752  0.8477 0.6777 0.6856]\n",
            " [0.6386 0.8721 0.6041 0.6138]\n",
            " [0.6618 0.8579 0.627  0.7004]\n",
            " [0.6423 0.8141 0.5846 0.6123]\n",
            " [0.603  0.771  0.6075 0.6504]\n",
            " [0.6116 0.7795 0.6068 0.6504]\n",
            " [0.6265 0.8585 0.6318 0.6311]\n",
            " [0.8811 0.5813 0.8817 0.6925]\n",
            " [0.8842 0.7772 0.735  0.7611]\n",
            " [0.7077 0.897  0.5979 0.6868]\n",
            " [0.6308 0.6044 0.7023 0.8276]\n",
            " [0.7475 0.8964 0.6837 0.6733]\n",
            " [0.5923 0.8271 0.7147 0.6861]\n",
            " [0.8965 0.3676 0.4359 0.8919]\n",
            " [0.7678 0.8472 0.4404 0.7074]\n",
            " [0.7697 0.7353 0.4535 0.8524]\n",
            " [0.5639 0.882  0.5484 0.7403]\n",
            " [0.9662 0.6569 0.7031 0.4784]\n",
            " [0.6319 0.6634 0.6983 0.6715]\n",
            " [0.9755 0.8558 0.0117 0.36  ]\n",
            " [0.73   0.1716 0.521  0.0543]\n",
            " [0.2    0.0185 0.8039 0.2239]\n",
            " [0.4044 0.8558 0.7302 0.1884]\n",
            " [0.2369 0.8664 0.6053 0.2379]\n",
            " [0.9342 0.614  0.5356 0.5899]\n",
            " [1.0062 0.3736 0.4918 0.4458]\n",
            " [0.5083 0.4461 0.4538 0.5692]\n",
            " [0.2274 0.2544 0.058  0.4344]\n",
            " [0.3118 0.5934 0.3791 0.2467]\n",
            " [0.0767 0.2324 0.7973 0.4537]\n",
            " [0.5366 0.8967 0.9903 0.2169]\n",
            " [0.6878 0.2633 0.0207 0.855 ]\n",
            " [0.32   0.3835 0.5883 0.831 ]\n",
            " [0.6306 1.2129 0.2735 0.8651]\n",
            " [0.376  0.5006 0.3941 0.3005]\n",
            " [0.4309 0.5434 0.3392 0.3244]\n",
            " [0.5749 0.1593 0.2695 0.4418]\n",
            " [0.3742 0.4636 0.2776 0.5868]\n",
            " [0.8639 0.1175 0.5174 0.1321]\n",
            " [0.7169 0.3961 0.5654 0.1833]\n",
            " [0.1448 0.4881 0.3556 0.9404]\n",
            " [0.7653 0.7487 0.9037 0.0834]]\n"
          ]
        }
      ]
    }
  ]
}