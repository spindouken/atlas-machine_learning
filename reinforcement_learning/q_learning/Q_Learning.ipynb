{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DnY2hRZJwLOJ"
      ],
      "authorship_tag": "ABX9TyMG3/5eyMUlQkhWTV2C7go5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spindouken/atlas-machine_learning/blob/main/reinforcement_learning/Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intro/Project/Game Explanation"
      ],
      "metadata": {
        "id": "DnY2hRZJwLOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project was finalized 1/10/2024.<br>\n",
        "Author: Mason Counts"
      ],
      "metadata": {
        "id": "Iezxp7x-VI2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "gymnasium frozen lake documentation:\n",
        "https://gymnasium.farama.org/environments/toy_text/frozen_lake/"
      ],
      "metadata": {
        "id": "P1A1O8wbVDyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of frozen lake** from gym doc:<br>\n",
        "The game starts with the player at location [0,0] of the frozen lake grid world with the goal located at far extent of the world e.g. [3,3] for the 4x4 environment.\n",
        "\n",
        "Holes in the ice are distributed in set locations when using a pre-determined map or in random locations when a random map is generated.\n",
        "\n",
        "The player makes moves until they reach the goal or fall in a hole.\n",
        "\n",
        "The lake is slippery (unless disabled) so the player may move perpendicular to the intended direction sometimes (see is_slippery).\n",
        "\n",
        "Randomly generated worlds will always have a path to the goal."
      ],
      "metadata": {
        "id": "opzVASrpVXr3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project Explanation:\n",
        "The point of this project is to gain some experience implementing and demonstrating a fundamental reinforcement learning (RL) algorithm, Q-learning, using the OpenAI Gym's FrozenLakeEnv. The goal is to train an agent to navigate a grid-like environment (the frozen lake) to reach a goal while avoiding pitfalls (holes). Here is some more information/explanation about the project tasks:\n",
        "\n",
        "0. The FrozenLakeEnv Environment\n",
        "Description: This environment is a grid of blocks, where each block can be either start (S), goal (G), safe (F), or hole (H). The agent starts at S and aims to reach G without falling into H.\n",
        "Challenge: The environment can be deterministic or stochastic ('slippery'). In a slippery environment, the agent's actions don't always result in the intended outcomes, mimicking the uncertainty of a real frozen lake.\n",
        "<br>\n",
        "1. Q-Learning Algorithm\n",
        "Purpose: Q-learning is a model-free reinforcement learning algorithm used to find an optimal action-selection policy for any given finite Markov decision process (MDP). It works by learning an action-value function that ultimately gives the expected utility of taking a given action in a given state and following the optimal policy thereafter.\n",
        "Application: In this project, Q-learning is used to train an agent to navigate the FrozenLakeEnv. The agent learns from its environment by interacting with it and receiving feedback in the form of rewards.\n",
        "<br>\n",
        "3. Training the Agent (train function)\n",
        "Process: The agent is trained over many episodes. In each episode, the agent makes decisions based on the current Q-table, observes the outcomes, and updates the Q-table using the Q-learning algorithm.\n",
        "Outputs: The training process outputs an updated Q-table, which represents the learned policy, and a list of total rewards per episode, which is a measure of how well the agent is learning over time.\n",
        "<br>\n",
        "4. Testing the Agent (play function)\n",
        "Purpose: After training, the play function is used to test the performance of the trained agent. It uses the learned Q-table to navigate the environment, always choosing the action with the highest Q-value.\n",
        "Output: The function displays each state of the board and the chosen action, providing a visual representation of the agent's path through the environment. The total reward for the episode is returned, indicating the success of the agent in reaching the goal without falling into holes.\n",
        "Significance of the Project\n",
        "Understanding RL Concepts: This project is a practical demonstration of key reinforcement learning concepts like exploration vs. exploitation, learning rate, discount factor, and the balance between them."
      ],
      "metadata": {
        "id": "VpA2PqMAZrM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code begins!"
      ],
      "metadata": {
        "id": "j53P0qQ4wGlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "def load_frozen_lake(desc=None, map_name=None, is_slippery=False):\n",
        "    \"\"\"\n",
        "    loads the pre-made FrozenLakeEnv evnironment from OpenAIâ€™s gym\n",
        "\n",
        "    desc: either None or a list of lists containing\n",
        "      a custom description of the map to load for the environment\n",
        "    map_name: either None or a string containing\n",
        "      the pre-made map to load\n",
        "      Note: If both desc and map_name are None,\n",
        "        the environment will load a randomly generated 8x8 map\n",
        "    is_slippery: a boolean to determine if the ice is slippery\n",
        "\n",
        "    Returns: the environment\n",
        "    \"\"\"\n",
        "    env = gym.make('FrozenLake-v1', desc=desc, map_name=map_name, is_slippery=is_slippery)\n",
        "    return env"
      ],
      "metadata": {
        "id": "E8SWvOpUQsu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)\n",
        "env = load_frozen_lake()\n",
        "print(env.desc)\n",
        "print(env.P[0][0])\n",
        "env = load_frozen_lake(is_slippery=True)\n",
        "print(env.desc)\n",
        "print(env.P[0][0])\n",
        "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
        "env = load_frozen_lake(desc=desc)\n",
        "print(env.desc)\n",
        "env = load_frozen_lake(map_name='4x4')\n",
        "print(env.desc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1g97P4O-Iik6",
        "outputId": "17f80ecd-d8fc-4cf0-8eb1-7ac49fce7531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[b'S' b'F' b'F' b'F' b'F' b'F' b'F' b'H']\n",
            " [b'H' b'F' b'F' b'F' b'F' b'H' b'F' b'F']\n",
            " [b'F' b'H' b'F' b'H' b'H' b'F' b'F' b'F']\n",
            " [b'F' b'F' b'F' b'H' b'F' b'F' b'F' b'F']\n",
            " [b'F' b'F' b'F' b'F' b'F' b'F' b'H' b'F']\n",
            " [b'F' b'F' b'F' b'F' b'F' b'F' b'F' b'F']\n",
            " [b'F' b'F' b'F' b'F' b'H' b'F' b'F' b'F']\n",
            " [b'F' b'F' b'F' b'F' b'F' b'F' b'F' b'G']]\n",
            "[(1.0, 0, 0.0, False)]\n",
            "[[b'S' b'F' b'H' b'F' b'H' b'F' b'H' b'F']\n",
            " [b'H' b'F' b'F' b'F' b'F' b'F' b'F' b'F']\n",
            " [b'F' b'F' b'F' b'F' b'F' b'F' b'F' b'F']\n",
            " [b'F' b'H' b'F' b'F' b'F' b'F' b'F' b'F']\n",
            " [b'F' b'F' b'H' b'F' b'F' b'F' b'F' b'H']\n",
            " [b'F' b'F' b'F' b'F' b'F' b'H' b'F' b'H']\n",
            " [b'F' b'F' b'H' b'F' b'H' b'F' b'H' b'F']\n",
            " [b'F' b'F' b'H' b'F' b'F' b'F' b'F' b'G']]\n",
            "[(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 8, 0.0, True)]\n",
            "[[b'S' b'F' b'F']\n",
            " [b'F' b'H' b'H']\n",
            " [b'F' b'F' b'G']]\n",
            "[[b'S' b'F' b'F' b'F']\n",
            " [b'F' b'H' b'F' b'H']\n",
            " [b'F' b'F' b'F' b'H']\n",
            " [b'H' b'F' b'F' b'G']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To implement the q_init function, we need to initialize a Q-table for the reinforcement learning agent in the FrozenLake environment. The Q-table is a matrix where each row represents a state in the environment, and each column represents an action the agent can take in that state. The values in the table are initialized to zero and will be updated during the learning process."
      ],
      "metadata": {
        "id": "iOR6LoAATma7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "initializes the Q-table\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def q_init(env):\n",
        "    \"\"\"\n",
        "    initializes the Q-table for the FrozenLakeEnv environment\n",
        "\n",
        "    env: the FrozenLakeEnv instance\n",
        "\n",
        "    Returns: the Q-table as a numpy.ndarray of zeros\n",
        "    \"\"\"\n",
        "    # retrieve the number of states from the observations space\n",
        "    stateCount = env.observation_space.n\n",
        "\n",
        "    # retrieve the number of actions from the action space\n",
        "    actionCount = env.action_space.n\n",
        "\n",
        "    # initialize Qtable with zeros\n",
        "    Qtable = np.zeros((stateCount, actionCount))\n",
        "\n",
        "    # the Q table will have number of rows = # states\n",
        "    #  and number of columns = # actions\n",
        "    return Qtable"
      ],
      "metadata": {
        "id": "868NvZnHIj50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.observation_space.n)\n",
        "print(env.action_space.n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge2YBHm1-3-S",
        "outputId": "fca73bc5-8513-4f0f-c1c4-585f6c9e4f24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = load_frozen_lake()\n",
        "Q = q_init(env)\n",
        "print('Q-Table Total States (map size(ex. 8x8 = 64)):')\n",
        "print(Q.shape)\n",
        "env = load_frozen_lake(is_slippery=True)\n",
        "Q = q_init(env)\n",
        "print('Q-Table Transition Probabilities (same as Q-Table size):')\n",
        "print(Q.shape)\n",
        "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
        "env = load_frozen_lake(desc=desc)\n",
        "Q = q_init(env)\n",
        "print('Q-table generation for 3x3 map:')\n",
        "print(Q.shape)\n",
        "env = load_frozen_lake(map_name='4x4')\n",
        "Q = q_init(env)\n",
        "print('Q-table generation for 4x4 map:')\n",
        "print(Q.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcBOgX-9Ijov",
        "outputId": "2153e28c-303f-4e09-b6ab-29bec0821034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-Table Total States (map size(ex. 8x8 = 64)):\n",
            "(64, 4)\n",
            "Q-Table Transition Probabilities (same as Q-Table size):\n",
            "(64, 4)\n",
            "Q-table generation for 3x3 map:\n",
            "(9, 4)\n",
            "Q-table generation for 4x4 map:\n",
            "(16, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "uses epsilon-greedy to determine the next action\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def epsilon_greedy(Q, state, epsilon):\n",
        "    \"\"\"\n",
        "    uses epsilon-greedy to determine the next action\n",
        "    epsilon_greedy chooses between explore or exploit according to the epsilon parameter\n",
        "    epsilon is the probability to select a random action\n",
        "\n",
        "    If exploring (p < epsilon): select a random action\n",
        "      using numpy.random.randint().\n",
        "    If exploiting (p >= epsilon): select the action\n",
        "      with the highest Q-value for the current state.\n",
        "    Returning the Action Index: The function returns\n",
        "      the index of the chosen action.\n",
        "\n",
        "    Q is a numpy.ndarray containing the q-table\n",
        "    state is the current state of the agent in the environment\n",
        "    epsilon is the epsilon to use for the calculation of the probability of selecting a random action\n",
        "    You should sample p with numpy.random.uniformn to determine if your algorithm should explore or exploit\n",
        "    If exploring, you should pick the next action with numpy.random.randint from all possible actions\n",
        "    Returns: the index of the chosen action\n",
        "    \"\"\"\n",
        "    # will agent explore or exploit?\n",
        "    if np.random.uniform(0, 1) < epsilon:\n",
        "        # explore: choose a random action\n",
        "        chosenAction = np.random.randint(Q.shape[1])\n",
        "    else:\n",
        "        # exploit: choose the best action based on current Q-table\n",
        "        chosenAction = np.argmax(Q[state])\n",
        "\n",
        "    return chosenAction"
      ],
      "metadata": {
        "id": "FYhhMqwRImqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
        "env = load_frozen_lake(desc=desc)\n",
        "Q = q_init(env)\n",
        "Q[7] = np.array([0.5, 0.7, 1, -1])\n",
        "np.random.seed(0)\n",
        "print('Testing epsilon greedy function with custom 3x3 env w/ zerod Q-table except for state 7:')\n",
        "\n",
        "print(epsilon_greedy(Q, 7, 0.5))\n",
        "print('The output is 2, meaning eps greedy chose to exploit')\n",
        "np.random.seed(1)\n",
        "print(epsilon_greedy(Q, 7, 0.5))\n",
        "print('The output is 0, meaning eps greedy chose to explore')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nzyWw6dIocG",
        "outputId": "377dde87-58d8-4781-9acb-1ce4feef820e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing epsilon greedy function with custom 3x3 env w/ zerod Q-table except for state 7:\n",
            "2\n",
            "The output is 2, meaning eps greedy chose to exploit\n",
            "0\n",
            "The output is 0, meaning eps greedy chose to explore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterate over the number of episodes.\n",
        "For each episode, reset the environment and iterate for a maximum number of steps.\n",
        "Use the epsilon-greedy strategy to choose an action.\n",
        "Take the action and observe the new state and reward.\n",
        "Update the Q-table using the Q-learning formula.\n",
        "If the agent falls into a hole, update the reward to -1.\n",
        "Update epsilon using the decay rate.\n",
        "Store the total reward for each episode.<br>\n",
        "**Q-learning Update Formula:**\n",
        "Q[state, action] = Q[state, action] + alpha * (reward + gamma * max(Q[new_state]) - Q[state, action])\n",
        "<br>\n",
        "**Return:**\n",
        "the updated Q-table and the list of total rewards per episode."
      ],
      "metadata": {
        "id": "JyDpe4KlU5W2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "performs Q-learning\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def train(\n",
        "    env,\n",
        "    Q,\n",
        "    episodes=5000,\n",
        "    max_steps=100,\n",
        "    alpha=0.1,\n",
        "    gamma=0.99,\n",
        "    epsilon=1,\n",
        "    min_epsilon=0.1,\n",
        "    epsilon_decay=0.05,\n",
        "):\n",
        "    \"\"\"\n",
        "    performs Q-learning\n",
        "\n",
        "    env is the FrozenLakeEnv instance\n",
        "    Q is a numpy.ndarray containing the Q-table\n",
        "    episodes is the total number of episodes to train over\n",
        "    max_steps is the maximum number of steps per episode\n",
        "    alpha is the learning rate\n",
        "    gamma is the discount rate\n",
        "    epsilon is the initial threshold for epsilon greedy\n",
        "    min_epsilon is the minimum value that epsilon should decay to\n",
        "    epsilon_decay is the decay rate for updating epsilon between episodes\n",
        "    When the agent falls in a hole, the reward should be updated to be -1\n",
        "    Returns: Q, total_rewards\n",
        "    Q is the updated Q-table\n",
        "    total_rewards is a list containing the rewards per episode\n",
        "    \"\"\"\n",
        "    total_rewards = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            if np.random.uniform(0, 1) < epsilon:\n",
        "                action = np.random.randint(env.action_space.n)\n",
        "            else:\n",
        "                action = np.argmax(Q[state])\n",
        "\n",
        "            new_state, reward, done, info = env.step(action)\n",
        "\n",
        "            # update reward when falling in a hole\n",
        "            if done and reward == 0:\n",
        "                reward = -1\n",
        "\n",
        "            # Q-learning formula\n",
        "            Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[new_state]) - Q[state, action])\n",
        "\n",
        "            total_reward += reward\n",
        "            state = new_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        epsilon = max(min_epsilon, epsilon - epsilon_decay)\n",
        "        total_rewards.append(total_reward)\n",
        "\n",
        "    return Q, total_rewards"
      ],
      "metadata": {
        "id": "EwfQlkEBIpvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
        "env = load_frozen_lake(desc=desc)\n",
        "Q = q_init(env)\n",
        "\n",
        "Q, total_rewards  = train(env, Q)\n",
        "print(Q)\n",
        "split_rewards = np.split(np.array(total_rewards), 10)\n",
        "for i, rewards in enumerate(split_rewards):\n",
        "    print((i+1) * 500, ':', np.mean(rewards))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKa6uxwHIpnp",
        "outputId": "42286240-630c-4f6a-991c-ea9767fec4af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.96059593 0.970299   0.95097357 0.96059591]\n",
            " [0.96059481 0.         0.02547319 0.        ]\n",
            " [0.25367681 0.         0.         0.        ]\n",
            " [0.97029691 0.9801     0.         0.9605959 ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.98009953 0.98009941 0.99       0.97029715]\n",
            " [0.98009832 0.98999981 1.         0.        ]\n",
            " [0.         0.         0.         0.        ]]\n",
            "500 : 0.892\n",
            "1000 : 0.958\n",
            "1500 : 0.924\n",
            "2000 : 0.932\n",
            "2500 : 0.942\n",
            "3000 : 0.948\n",
            "3500 : 0.95\n",
            "4000 : 0.924\n",
            "4500 : 0.958\n",
            "5000 : 0.944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "\n",
        "# how many games to have the agent play\n",
        "play_count = 3\n",
        "\n",
        "def render_game(env, state, grid_size):\n",
        "    \"\"\"\n",
        "    Custom render function for the game.\n",
        "    env: the game environment\n",
        "    state: current state of the agent\n",
        "    grid_size: size of the grid\n",
        "    \"\"\"\n",
        "    grid = np.array(env.desc).reshape(grid_size, grid_size)\n",
        "    row, col = state // grid_size, state % grid_size\n",
        "\n",
        "    for r in range(grid_size):\n",
        "        for c in range(grid_size):\n",
        "            if r == row and c == col:\n",
        "                print(\"\\033[91m{}\\033[00m\".format(grid[r, c].decode(\"utf-8\")), end=\" \")\n",
        "            else:\n",
        "                print(grid[r, c].decode(\"utf-8\"), end=\" \")\n",
        "        print(\"\")\n",
        "\n",
        "def play(env, Q, max_steps=100, grid_size=4):\n",
        "    \"\"\"\n",
        "    env is the FrozenLakeEnv instance\n",
        "    Q is a numpy.ndarray containing the Q-table\n",
        "    max_steps is the maximum number of steps in the episode\n",
        "    grid_size is the size of the grid\n",
        "    \"\"\"\n",
        "    for episode in range(play_count):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_rewards = 0\n",
        "        print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
        "        time.sleep(1)\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            clear_output(wait=True)\n",
        "            render_game(env, state, grid_size)\n",
        "            time.sleep(0.3)\n",
        "\n",
        "            action = np.argmax(Q[state])\n",
        "            new_state, reward, done, info = env.step(action)\n",
        "            total_rewards += reward\n",
        "\n",
        "            if done:\n",
        "                clear_output(wait=True)\n",
        "                render_game(env, state, grid_size)\n",
        "                if reward == 1:\n",
        "                    print(\"****The agent ascended!****\")\n",
        "                else:\n",
        "                    print(\"****The agent fell into the abyss!****\")\n",
        "                time.sleep(3)\n",
        "                break\n",
        "\n",
        "            state = new_state\n",
        "\n",
        "        print(f\"Episode: {episode + 1}, Total Rewards: {total_rewards}\\n\")\n",
        "\n",
        "    env.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "OFpoMIApuNNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "np.random.seed(0)\n",
        "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
        "env = load_frozen_lake(desc=desc)\n",
        "Q = q_init(env)\n",
        "\n",
        "Q, total_rewards = train(env, Q)\n",
        "print(\"Total rewards from training:\", total_rewards)\n",
        "\n",
        "# Play the game\n",
        "play(env, Q, max_steps=100, grid_size=3)"
      ],
      "metadata": {
        "id": "id7LMlaNIrj5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e20963de-ddf6-4909-b29b-5b281c44ad40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S F F \n",
            "F H H \n",
            "F \u001b[91mF\u001b[00m G \n",
            "****The agent ascended!****\n",
            "Episode: 3, Total Rewards: 1.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code below is an expansion of the project, building a random size map with a random construction for each agent run. Stochasticity was also introduced to the frozen spots in the lake with the is_slippery being set to true in the main file. The agent is not having such a fun time with such short training now."
      ],
      "metadata": {
        "id": "t8GhI604uXj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "def load_frozen_lake(desc=None, map_name=None, is_slippery=False):\n",
        "    \"\"\"\n",
        "    Loads a FrozenLake environment with a random map if desc and map_name are None.\n",
        "    \"\"\"\n",
        "    if desc is None and map_name is None:\n",
        "        # generate a random map between 4 - 8 grid size\n",
        "        size = np.random.choice([4, 5, 6, 7, 8])\n",
        "        desc = gym.envs.toy_text.frozen_lake.generate_random_map(size=size)\n",
        "    env = gym.make('FrozenLake-v1', desc=desc, map_name=map_name, is_slippery=is_slippery)\n",
        "    return env\n"
      ],
      "metadata": {
        "id": "nWGMjJWcY9Gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    np.random.seed(0)\n",
        "\n",
        "    # number of maps to generate\n",
        "    num_maps = 5\n",
        "\n",
        "    for i in range(num_maps):\n",
        "        print(f\"Map {i+1}:\")\n",
        "\n",
        "        # load a random map\n",
        "        env = load_frozen_lake(is_slippery=True)\n",
        "        Q = q_init(env)\n",
        "\n",
        "        # train the agent\n",
        "        Q, total_rewards = train(env, Q)\n",
        "        print(\"Total rewards from training:\", total_rewards)\n",
        "\n",
        "        # play the game\n",
        "        play(env, Q, max_steps=1000, grid_size=env.nrow)\n",
        "\n",
        "        print(\"\\n\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "11K6bzIbY-Yp",
        "outputId": "1a58d0e3-04f7-46f7-f64a-7e7a0899cbf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S H F F H F \n",
            "F F F F F F \n",
            "\u001b[91mF\u001b[00m H H H F H \n",
            "H F H H F H \n",
            "F F H F F H \n",
            "F F F F F G \n",
            "****The agent fell into the abyss!****\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-aa3f168d9db0>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-aa3f168d9db0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Play the game\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnrow\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Assuming env.nrow gives the size of the grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-6a5e122bb2a4>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(env, Q, max_steps, grid_size)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"****The agent fell into the abyss!****\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}
